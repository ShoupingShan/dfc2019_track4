# -*-coding:utf-8-*-
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

#labels表示你不同类别的代号，比如这里的demo中有13个类别
# labels = ['0', '2', '5', '6', '9']

#y_true代表真实的label值 y_pred代表预测得到的lavel值
Num = '314'
y_true = np.loadtxt('/home/shp/Documents/Code/Python/Contest/dfc2019/track4/pointnet2/data/dfc/inference_data/gt/JAX_'+Num+'_CLS.txt')
y_pred = np.loadtxt('/home/shp/Documents/Code/Python/Contest/dfc2019/track4/pointnet2/data/dfc/inference_data/out/JAX_'+Num+'_CLS.txt')
labels_index = np.unique([y_true,y_pred])
# LABELS_OBJ = {0:"Undefined",2: "Ground", 5: "High Vegetation", 6: "Building", 9: "Water", 17: "Bridge Deck"}
# LABELS = sorted(LABELS_OBJ.keys())
# LABEL_INDEXES = dict([(label, index) for index, label in enumerate(sorted(LABELS_OBJ.keys()))])
labels = []
for item in labels_index:
    if int(item) is 0:
        labels.append('Undefined')
    elif not int(item) - 2:
        labels.append('Ground')
    elif not int(item) - 5:
        labels.append('High Vegetation')
    elif not int(item) - 6:
        labels.append('Building')
    elif not int(item) - 9:
        labels.append('Water')
    else:
        labels.append('Bridge Deck')
tick_marks = np.array(range(len(labels))) + 0.5


def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.binary):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    xlocations = np.array(range(len(labels )- 1))
    plt.xticks(xlocations, labels[1:], rotation=90)
    plt.yticks(xlocations, labels[1:])
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def score_predictions(matrix):
    """
    Calculates the number of true positives, false negatives and false positives for each class in a confusion matrix
    :param matrix: Confusion matrix for a prediction event
    :return: An array of ClassificationScores, one for each classification label
    """
    per_class = []
    for current_class_index in range(len(labels) - 1):
        true_positives = matrix[current_class_index][current_class_index]
        # false negatives are the points where the truth is the class, but the prediction is not (so sum across the
        # row and subtract off true positives)
        false_negatives = np.sum(matrix[current_class_index, :]) - true_positives
        # false negatives are the points where the prediction is the class, but the truth is not (so sum across the
        # column and subtract off true positives)
        false_positives = np.sum(matrix[:, current_class_index]) - true_positives
        per_class.append([true_positives, false_negatives, false_positives])

    return per_class

def get_iou(perclass_item):
    denominator = perclass_item[0] + perclass_item[1] + perclass_item[2]
    if denominator == 0:
        return 1
    return perclass_item[0] / denominator

def get_mean_intersection_over_union(scores):
    """
    MIOU generated by averaging IOUs of each class, unweighted
    :param scores: an array of ClassificationScore objects
    :return: 0->1 where 1 implies perfect intersection
    """
    iou = 0
    for score in scores:
        iou += get_iou(score)
    if len(scores) < 1:
        return 0
    return iou / len(scores)

def report_scores(confusion_matrix):
    prediction_scores = score_predictions(confusion_matrix)
    print("MIOU: {}".format(get_mean_intersection_over_union(prediction_scores)))
    for index, prediction_score in enumerate(prediction_scores):
        print("Class{%s} : IOU: %.4f"%(labels[index+1], get_iou(prediction_score)))

cm = confusion_matrix(y_true, y_pred)
report_scores(cm[1:,1:])
np.set_printoptions(precision=7)
x = cm.sum(axis=1)[:, np.newaxis]
for i in range(len(x)):
    if not x[i]:
        x[i] = 1
cm_normalized = cm.astype('float') / x
cm_normalized = cm_normalized[1:, 1:]
print('Confusion Metrics\n',cm[1:,1:])
plt.figure(figsize=(12, 8), dpi=120)

ind_array = np.arange(len(labels)-1)
x, y = np.meshgrid(ind_array, ind_array)

for x_val, y_val in zip(x.flatten(), y.flatten()):
    c = cm_normalized[y_val][x_val]
    if c > 0.000000001:
        plt.text(x_val, y_val, "%0.7f" % (c,), color='red', fontsize=10, va='center', ha='center')
# offset the tick
plt.gca().set_xticks(tick_marks, minor=True)
plt.gca().set_yticks(tick_marks, minor=True)
plt.gca().xaxis.set_ticks_position('none')
plt.gca().yaxis.set_ticks_position('none')
plt.grid(True, which='minor', linestyle='-')
plt.gcf().subplots_adjust(bottom=0.15)

plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')
# show confusion matrix
plt.savefig('confusion_matrix.png', format='png')
plt.show()
